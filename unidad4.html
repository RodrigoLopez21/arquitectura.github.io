<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Arquitectura</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <h1>Arquitectura de Computadoras</h1>
    <nav>
        <ul>
            <li><a href="index.html">Inicio</a></li>
            <li class="dropdown">
                <span>Unidades</span>
                <ul class="dropdown-content">
                    <li><a href="unidad1.html">Unidad 1</a></li>
                    <li><a href="unidad2.html">Unidad 2</a></li>
                    <li><a href="unidad3.html">Unidad 3</a></li>
                    <li><a href="unidad4.html">Unidad 4</a></li>
                </ul>
            </li>
            <li><a href="https://drive.google.com/drive/folders/180aHiosKg-V2U2eKhv3_3Zh13e5V7_TV?usp=sharing" target="_blank">Prácticas</a></li>
        </ul>
    </nav>
    <div class="contenido">
        <div class="Informacion">
            <div>
                <h2><br>Unidad 4</h2>
                <h2><br>4.1 Aspectos básicos de la computación paralela</h2>
                <p>
                    <br>La computación paralela se basa en la idea de dividir un problema en tareas más pequeñas y procesarlas de manera simultánea utilizando múltiples recursos de computación. Esto permite un procesamiento más rápido y eficiente en comparación con los enfoques secuenciales tradicionales. Algunos aspectos fundamentales de la computación paralela incluyen la sincronización de tareas, la comunicación entre procesos y la gestión de recursos.
                    <br><br>
                </p>
                <div class="image-container">
                    <img src="imgs/Paralela.jpg" class="paralela">
                </div>
                <h2><br>4.2 Tipos de computación paralela</h2>
                <p>
                    <br>Esta materia es fundamental en la carrera de Ingeniería en Sistemas Computacionales
                    porque proporciona los fundamentos teóricos y prácticos necesarios para entender cómo 
                    funcionan los sistemas informáticos a nivel de hardware. Esto incluye el diseño y la 
                    organización de componentes como procesadores, memoria, dispositivos de entrada/salida, etc.
                    <br><br>Esta página pertenece a Rodrigo López Rentería, estudiante en 4to semestre en Ingeniería 
                    en Sistemas Computacionales en el Instituto Tecnológico de Saltillo, en el menú de arriba 
                    podrás visualizar los temas de cada unidad así como las prácticas físicas que se realizaron.
                    <br><br>
                </p>
                <h2><br>4.2 Tipos de computación paralela</h2>
                <p>
                    <br>Existen varios tipos de computación paralela que se utilizan en diferentes contextos y escenarios. Algunos de los enfoques más comunes incluyen el procesamiento paralelo a nivel de bit, a nivel de instrucción, a nivel de datos y a nivel de tarea. Estos enfoques se diferencian en cómo se dividen y procesan las tareas y los datos.
                    <br><br>
                </p>
                <h2><br>4.2.1 Clasificacion</h2>
                <p>
                    <br>La clasificación de la computación paralela puede realizarse en función de la forma en que se dividen las tareas y los datos, así como de la forma en que se coordinan y comunican los procesos paralelos. Algunas clasificaciones comunes incluyen la computación paralela a nivel de bit, a nivel de instrucción, a nivel de datos y a nivel de tarea.
                    <br><br>
                </p>
                <h2><br>4.2.2 Arquitectura de computadores secuenciales</h2>
                <p>
                    <br>La arquitectura de computadores secuencial se refiere a los sistemas informáticos tradicionales en los que las instrucciones se ejecutan una tras otra en secuencia. Este tipo de arquitectura sigue siendo común en muchas computadoras personales y estaciones de trabajo.
                    <br><br>
                </p>
                <h2><br>4.2.3 Organización de direcciones de memoria</h2>
                <p>
                    <br>La organización de direcciones de memoria se refiere a cómo se asignan y acceden a las direcciones de memoria en un sistema de computación paralela. Esto incluye consideraciones como la memoria compartida, la memoria distribuida y las técnicas de direccionamiento utilizadas para acceder a los datos en paralelo.
                    <br><br>
                </p>
                <h2><br>4.3 Sistema de memoria compartida</h2>
                <p>
                    <br>Los sistemas de memoria compartida son un enfoque de computación paralela en el que múltiples procesadores acceden a una misma área de memoria compartida. Esto permite a los procesadores compartir datos y comunicarse de manera eficiente. Dentro de los sistemas de memoria compartida, existen dos tipos principales de redes: las redes de medio compartida y las redes conmutadas.
                    <br><br>
                </p>
                <h2><br>4.3.1.1 Redes de medio compartida</h2>
                <p>
                    <br>Las redes de medio compartida son un tipo de arquitectura de memoria compartida en la que los procesadores se conectan físicamente a un bus compartido o a una red de interconexión. Los procesadores pueden leer y escribir en la memoria compartida a través de este medio compartido.
                    <br><br>
                </p>
                <h2><br>4.3.1.2 Redes conmutadas</h2>
                <p>
                    <br>Las redes conmutadas, por otro lado, utilizan interruptores o conmutadores para establecer conexiones entre los procesadores y la memoria compartida. Estas redes ofrecen una mayor escalabilidad y capacidad de comunicación en comparación con las redes de medio compartida.
                    <br><br>
                </p>
                <div class="image-container">
                    <img src="imgs/Conmutacion.gif" class="conmutacion">
                </div>
                <h2><br>4.4 Sisitemas de memoria construida</h2>
                <p>
                    <br>Los sistemas de memoria construida son una forma de organización de la memoria en la computación paralela en la que cada procesador tiene su propia memoria local. Esto permite una mayor independencia entre los procesadores y reduce la necesidad de acceder a una memoria compartida.
                    <br><br>
                </p>
                <h2><br>4.5 Casos de estudio</h2>
                <p>
                    <br>Por numerosos motivos, el procesamiento distribuido se ha convertido en un área de gran importancia e interés dentro de la Ciencia de la Computación, produciendo profundas transformaciones en las líneas de I/D.
                    <br><br>Interesa realizar investigación en la especificación, transformación, optimización y evaluación de algoritmos distribuidos y paralelos. Esto incluye el diseño y desarrollo de sistemas paralelos, la transformación de algoritmos secuenciales en paralelos, y las métricas de evaluación de performance sobre distintas plataformas de soporte (hardware y software). Más allá de las mejoras constantes en las arquitecturas físicas de soporte, uno de los mayores desafíos se centra en cómo aprovechar al máximo la potencia de las mismas.
                    <h3><br>Líneas De Investigación Y Desarrollo</h3>
                    <br>-Paralelización de algoritmos secuenciales. Diseño y optimización de algoritmos.
                    <br><br>-Arquitecturas multicore y multithreading en multicore.
                    <br><br>-Arquitecturas multiprocesador.
                    <br><br>-Modelos de representación y predicción de performance de algoritmos paralelos.
                    <br><br>-Mapping y scheduling de aplicaciones paralelas sobre distintas arquitecturas multiprocesador.
                    <br><br>-Métricas del paralelismo. Speedup, eficiencia, rendimiento, granularidad, superlinealidad.
                    <br><br>-Balance de carga estático y dinámico. Técnicas de balanceo de carga.
                    <br><br>-Análisis de los problemas de migración y asignación óptima de procesos y datos a procesadores. Migración dinámica.
                    <br><br>-Patrones de diseño de algoritmos paralelos.
                    <br><br>-Escalabilidad de algoritmos paralelos en arquitecturas multiprocesador distribuidas.
                    <br><br>-Implementación de soluciones sobre diferentes modelos de arquitectura homogéneas y heterogéneas (multicores, clusters, multiclusters y grid). Ajuste del modelo de software al modelo de hardware, a fin de optimizar el sistema paralelo.
                    <br><br>-Evaluación de performance.
                    <br><br>-Laboratorios remotos para el acceso transparente a recursos de cómputo paralelo.
                    <br><br><br>
                </p>
            </div>
        </div>
    </div>
</body>
</html